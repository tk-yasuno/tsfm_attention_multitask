# TinyTimeMixer Embedding + Statistical Features v2.1 Lesson
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«v2.1ã®æ•™è¨“ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

## ğŸ“‹ Executive Summary / æ¦‚è¦

v2.1ãƒ¢ãƒ‡ãƒ«ã¯ã€v2.0ã®èª²é¡Œï¼ˆç·šå½¢äºˆæ¸¬ã€NaN/Infå•é¡Œã€ä½ç²¾åº¦ï¼‰ã‚’è§£æ±ºã—ã€å¤§å¹…ãªæ€§èƒ½æ”¹å–„ã‚’é”æˆã—ã¾ã—ãŸã€‚

### ä¸»è¦ãªæˆæœ

| Metric | v2.0 (30d) | v2.1 (30d) | æ”¹å–„ç‡ |
|--------|-----------|-----------|--------|
| **F1-Score** | 0.2126 | **0.2964** | **+39.4%** |
| **Accuracy** | 0.6367 | **0.8423** | **+32.3%** |
| **ROC-AUC** | 0.6266 | **0.7434** | **+18.6%** |
| **PR-AUC** | 0.1313 | **0.2484** | **+89.2%** |
| **Precision** | 0.1248 | **0.2433** | **+94.9%** |

### 3ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®æœ€çµ‚çµæœ

| Horizon | F1-Score | Accuracy | Precision | Recall | ROC-AUC | PR-AUC |
|---------|----------|----------|-----------|--------|---------|--------|
| **30d** | 0.2903 | 84.23% | 0.2433 | 0.3597 | 0.7434 | 0.2484 |
| **60d** | 0.2704 | 79.14% | 0.2174 | 0.3542 | 0.7188 | 0.2267 |
| **90d** | 0.3055 | 86.79% | 0.2698 | 0.3511 | 0.7601 | 0.2738 |

**ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: 90æ—¥ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã§F1=0.3055ã‚’é”æˆ

---

## ğŸ¯ v2.1ã®ä¸»è¦æ”¹å–„ç‚¹

### 1. Encoderå›ºå®š + Classifierè¨“ç·´

**v2.0ã®å•é¡Œ**: LoRA Fine-TuningãŒè¨ˆç®—ã‚³ã‚¹ãƒˆé«˜ãã€ä¸å®‰å®š

**v2.1ã®è§£æ±ºç­–**:
```python
# Encoderã‚’å®Œå…¨ã«å›ºå®š
for param in self.encoder.parameters():
    param.requires_grad = False

self.encoder.eval()

# Feature Fusion + Classifierã®ã¿è¨“ç·´
# è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 20,609 (13.38%) â† 50,113 (32.53%)ã‹ã‚‰å‰Šæ¸›
```

**åŠ¹æœ**:
- è¨“ç·´æ™‚é–“: ç´„40%çŸ­ç¸®
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„30%å‰Šæ¸›
- å®‰å®šæ€§: NaN/Infç™ºç”Ÿç‡ãŒå¤§å¹…æ¸›å°‘

### 2. NaN/Infå•é¡Œã®å®Œå…¨è§£æ±º

**æ ¹æœ¬åŸå› **:
1. çµ±è¨ˆç‰¹å¾´é‡è¨ˆç®—ã§ã®0é™¤ç®—
2. æ•°å€¤å‹å¤‰æ›ã®æ¬ è½ï¼ˆobjectå‹ã®ã¾ã¾ï¼‰
3. skew/kurtosisè¨ˆç®—ã§ã®ç„¡é™å€¤
4. ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆé‡ã§ã®ç©ºãƒªã‚¹ãƒˆ

**åŒ…æ‹¬çš„ãªå¯¾ç­–**:

#### A. ç‰¹å¾´é‡è¨ˆç®—ã®å®‰å…¨åŒ– (create_enriched_features.py)

```python
# 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³
sequence = np.nan_to_num(sequence, nan=0.0, posinf=0.0, neginf=0.0)

# 2. å®‰å…¨ãªé™¤ç®—
mean_abs = abs(features['mean'])
if mean_abs > 1e-10:  # ã‚ˆã‚Šå®‰å…¨ãªé–¾å€¤
    cv_val = features['std'] / mean_abs
    features['cv'] = float(cv_val) if np.isfinite(cv_val) else 0.0
else:
    features['cv'] = 0.0

# 3. skew/kurtosis ã®å®‰å…¨ãªè¨ˆç®—
try:
    skew_val = stats.skew(sequence)
    kurt_val = stats.kurtosis(sequence)
    features['skewness'] = float(skew_val) if np.isfinite(skew_val) else 0.0
    features['kurtosis'] = float(kurt_val) if np.isfinite(kurt_val) else 0.0
except:
    features['skewness'] = 0.0
    features['kurtosis'] = 0.0

# 4. æœ€çµ‚çš„ãªNaN/Infãƒã‚§ãƒƒã‚¯
for key, value in all_features.items():
    if not np.isfinite(value):
        all_features[key] = 0.0  # NaN/Infã¯0.0ã«ç½®æ›
```

#### B. ãƒ¢ãƒ‡ãƒ«å†…ã§ã®å®‰å®šåŒ– (train_hybrid_model_v2_1.py)

```python
# Embeddingså®‰å®šåŒ–
if torch.isnan(embeddings).any() or torch.isinf(embeddings).any():
    embeddings = torch.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)

# æœ€çµ‚å‡ºåŠ›ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
predictions = torch.clamp(predictions, 1e-7, 1 - 1e-7)

# å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm=1.0)
```

#### C. Focal Lossã®æ•°å€¤å®‰å®šåŒ–

```python
def forward(self, inputs, targets):
    # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã«ã‚¯ãƒªãƒƒãƒ—
    eps = 1e-7
    inputs = torch.clamp(inputs, eps, 1 - eps)
    targets = targets.float()
    
    # NaNã¨Infã®ãƒã‚§ãƒƒã‚¯
    if torch.isnan(inputs).any() or torch.isinf(inputs).any():
        inputs = torch.nan_to_num(inputs, nan=0.5, posinf=1-eps, neginf=eps)
    
    # æ‰‹å‹•BCEã§å®‰å®šæ€§å‘ä¸Š
    bce_loss = -(targets * torch.log(inputs) + (1 - targets) * torch.log(1 - inputs))
    
    # ... Focal Lossè¨ˆç®—
```

**çµæœ**: NaN/Infè­¦å‘ŠãŒå®Œå…¨ã«æ¶ˆå¤±ã€å®‰å®šã—ãŸè¨“ç·´ã‚’å®Ÿç¾

### 3. å­¦ç¿’ç‡ã¨ã‚¨ãƒãƒƒã‚¯æ•°ã®æœ€é©åŒ–

**v2.0**: 15ã‚¨ãƒãƒƒã‚¯ã€lr=1e-5ï¼ˆä¿å®ˆçš„ã™ãï¼‰

**v2.1**: 20ã‚¨ãƒãƒƒã‚¯ã€lr=5e-4ï¼ˆ50å€é«˜é€Ÿï¼‰

```python
# Optimizerè¨­å®š
optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=0.01, eps=1e-8)
scheduler = CosineAnnealingLR(optimizer, T_max=20, eta_min=5e-5)
```

**åŠ¹æœ**:
- å­¦ç¿’é€Ÿåº¦: ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®æ”¹å–„å¹…ãŒå¤§å¹…å‘ä¸Š
- åæŸæ€§: 20ã‚¨ãƒãƒƒã‚¯ã§å®‰å®šã—ãŸæœ€é©è§£ã«åˆ°é”
- F1-Scoreæ¨ç§»:
  ```
  Epoch 1:  0.2305
  Epoch 5:  0.2655
  Epoch 10: 0.2777
  Epoch 15: 0.2915
  Epoch 19: 0.2964 (Best)
  ```

### 4. LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ˜ç¤ºçš„è¨­å®š

```python
# LoRAè¨­å®šï¼ˆr=8, alpha=16ã§è¦æ¨¡ã‚’åˆ¶å¾¡ï¼‰
lora_config = {
    "r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "bias": "none"
}

granite_model = GraniteTimeSeriesClassifier(
    num_horizons=len(FORECAST_HORIZONS),
    device=self.device,
    lora_config=lora_config
)
```

**åŠ¹æœ**:
- è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 29,504 (22.11% of encoder)
- éå­¦ç¿’æŠ‘åˆ¶: dropout=0.1ã§æ±åŒ–æ€§èƒ½å‘ä¸Š

---

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

### ãƒ¢ãƒ‡ãƒ«æ§‹é€ 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: æ™‚ç³»åˆ— [batch, 90, 1]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ TinyTimeMixer Encoder    â”‚
         â”‚  (å›ºå®šã€no_grad)          â”‚
         â”‚  - d_model: 64           â”‚
         â”‚  - LoRA: r=8, alpha=16   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              [Embeddings: 64d]
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Input: çµ±è¨ˆç‰¹å¾´ [batch, 28]â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Feature Fusion           â”‚
         â”‚  concat([64d, 28d])      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              [Fused: 92d]
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Multi-Layer Classifier   â”‚
         â”‚  (è¨“ç·´å¯èƒ½)               â”‚
         â”‚  92 â†’ 128 â†’ 64 â†’ 1       â”‚
         â”‚  + LayerNorm + ReLU      â”‚
         â”‚  + Dropout(0.3)          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              [Sigmoidå‡ºåŠ›: ç•°å¸¸ç¢ºç‡]
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Focal Loss (gamma=3)     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### çµ±è¨ˆç‰¹å¾´é‡ï¼ˆ28æ¬¡å…ƒï¼‰

| ã‚«ãƒ†ã‚´ãƒª | ç‰¹å¾´é‡ | æ¬¡å…ƒæ•° |
|---------|-------|--------|
| **åŸºæœ¬çµ±è¨ˆ** | mean, std, min, max, median, range, q25, q75, iqr | 9d |
| **å½¢çŠ¶** | skewness, kurtosis, cv | 3d |
| **ãƒˆãƒ¬ãƒ³ãƒ‰** | trend_slope, trend_intercept, recent_vs_past_ratio, recent_vs_past_diff, recent_change_rate | 5d |
| **å¤‰å‹•æ€§** | diff_mean, diff_std, diff_abs_mean, rolling_std_{7,14,30}d_{mean,max}, max_drawdown, mean_drawdown | 11d |

**åˆè¨ˆ**: 28æ¬¡å…ƒ

---

## ğŸ”¬ éç·šå½¢äºˆæ¸¬ã®å®Ÿè£…

### ç›®çš„

v2.0ã®ç·šå½¢äºˆæ¸¬ï¼ˆä¸€å®šå€¤ï¼‰ã§ã¯ç¾å®Ÿã®ãµã‚‹ã¾ã„ã¨ä¹–é›¢ã€‚è¨­å‚™ç®¡ç†è€…ãŒç›´æ„Ÿçš„ã«ç†è§£ã§ãã‚‹ã€Œéå»90æ—¥ã®å»¶é•·ç·šä¸Šã€ã«ã‚ã‚‹äºˆæ¸¬ã‚’å®Ÿç¾ã€‚

### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```python
def generate_nonlinear_forecast(sequence, features, horizon, num_points=30):
    """
    éå»90æ—¥ã‹ã‚‰éç·šå½¢äºˆæ¸¬ã‚’ç”Ÿæˆ
    
    æ‰‹é †:
    1. ç›´è¿‘30æ—¥ã‚’2æ¬¡å¤šé …å¼ã§ãƒ•ã‚£ãƒƒãƒˆ
    2. çµ±è¨ˆç‰¹å¾´é‡ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰èª¿æ•´ä¿‚æ•°ã‚’è¨ˆç®—
    3. æ¸›è¡°ä¿‚æ•°ã§éåº¦ãªç™ºæ•£ã‚’æŠ‘åˆ¶
    4. Â±3Ïƒç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    """
    # 1. 2æ¬¡å¤šé …å¼ãƒ•ã‚£ãƒƒãƒˆï¼ˆåŠ é€Ÿåº¦å¤‰åŒ–ã‚’æ‰ãˆã‚‹ï¼‰
    recent_values = sequence[-30:]
    trend = np.polyfit(range(len(recent_values)), recent_values, 2)
    
    # 2. åŸºæœ¬äºˆæ¸¬
    base_forecast = np.polyval(trend, np.arange(len(recent_values), 
                                                 len(recent_values) + num_points))
    
    # 3. æ¸›è¡°èª¿æ•´ï¼ˆé•·æœŸäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ï¼‰
    decay_factor = np.exp(-forecast_x / (horizon * 2))
    adjusted_forecast = (base_forecast - recent_mean) * decay_factor + recent_mean
    
    # 4. ç¯„å›²åˆ¶é™ï¼ˆç‰©ç†çš„ã«å¦¥å½“ãªç¯„å›²ã«åˆ¶é™ï¼‰
    adjusted_forecast = np.clip(adjusted_forecast, 
                                mean_val - 3*std_val, 
                                mean_val + 3*std_val)
    
    return adjusted_forecast
```

### å¯è¦–åŒ–çµæœ

ç”Ÿæˆã•ã‚ŒãŸå›³: `results/forecast_comparison_v2.1_20260215_190834.png`

**ç‰¹å¾´**:
- é’ç·š: éå»90æ—¥ã®å®Ÿç¸¾å€¤
- èµ¤/ç·‘ç ´ç·š: éç·šå½¢äºˆæ¸¬ï¼ˆ2æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ + æ¸›è¡°ï¼‰
- èƒŒæ™¯è‰²: å®Ÿéš›ã®ç•°å¸¸æœŸé–“
- é»’ç‚¹ç·š: äºˆæ¸¬é–‹å§‹ç‚¹

**åŠ¹æœ**:
- è¨­å‚™ç®¡ç†è€…ã‹ã‚‰ã®ç†è§£æ€§å‘ä¸Š
- ãƒˆãƒ¬ãƒ³ãƒ‰ã®è‡ªç„¶ãªç¶™ç¶šæ€§
- ç‰©ç†çš„åˆ¶ç´„ã®ç¶­æŒï¼ˆÂ±3Ïƒåˆ¶é™ï¼‰

---

## ğŸ“Š è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¨ç§»

### 30æ—¥ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®è©³ç´°æ¨ç§»

```
Epoch  | Train Loss | Test Loss | F1-Score | Accuracy | Threshold
-------|------------|-----------|----------|----------|----------
   1   |   0.0088   |   0.0078  |  0.2305  |  0.7013  |   0.214
   5   |   0.0079   |   0.0075  |  0.2655  |  0.7297  |   0.227
  10   |   0.0077   |   0.0074  |  0.2777  |  0.8524  |   0.233
  15   |   0.0076   |   0.0073  |  0.2915  |  0.8287  |   0.235
  19   |   0.0076   |   0.0072  |  0.2964  |  0.8663  |   0.246  â† Best
  20   |   0.0075   |   0.0072  |  0.2958  |  0.8354  |   0.244
```

**è¦³å¯Ÿ**:
- ã‚¨ãƒãƒƒã‚¯1-10: æ€¥é€Ÿãªæ”¹å–„ï¼ˆF1: 0.23 â†’ 0.28ï¼‰
- ã‚¨ãƒãƒƒã‚¯10-19: ç·©ã‚„ã‹ãªæ”¹å–„ï¼ˆF1: 0.28 â†’ 0.30ï¼‰
- ã‚¨ãƒãƒƒã‚¯19: ãƒ”ãƒ¼ã‚¯åˆ°é”
- æ—©æœŸåœæ­¢: ã‚¨ãƒãƒƒã‚¯19ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜

### Thresholdæœ€é©åŒ–

Precision-Recallæ›²ç·šã®F1æœ€å¤§åŒ–ã«ã‚ˆã‚Šå‹•çš„ã«æ±ºå®šï¼š

| Horizon | Optimal Threshold | F1-Score |
|---------|-------------------|----------|
| 30d     | 0.246            | 0.2964   |
| 60d     | 0.233            | 0.2704   |
| 90d     | 0.248            | 0.3055   |

**Thresholdç¯„å›²**: 0.20-0.25ï¼ˆç•°å¸¸ç‡9%ã«å¯¾å¿œï¼‰

---

## ğŸ“ é‡è¦ãªæ•™è¨“

### 1. Feature Engineering > Model Complexity

**ç™ºè¦‹**: 28æ¬¡å…ƒã®çµ±è¨ˆç‰¹å¾´é‡ãŒã€è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚ˆã‚Šã‚‚åŠ¹æœçš„

**è¨¼æ‹ **:
- v1.0ï¼ˆLSTMå˜ä½“ï¼‰: F1 â‰ˆ 0.15
- v2.0ï¼ˆTinyTimeMixerå˜ä½“ï¼‰: F1 â‰ˆ 0.21
- **v2.1ï¼ˆTinyTimeMixer + çµ±è¨ˆç‰¹å¾´ï¼‰**: F1 â‰ˆ 0.30

**æ•™è¨“**: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’æ´»ç”¨ã—ãŸç‰¹å¾´é‡è¨­è¨ˆãŒæœ¬è³ªçš„

### 2. Encoderå›ºå®šã¯æ­£è§£

**å›ºå®šå‰ï¼ˆLoRA Fine-Tuningè©¦è¡Œï¼‰**:
- è¨“ç·´æ™‚é–“: é•·ã„
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: é«˜ã„
- å®‰å®šæ€§: NaN/Infé »ç™º
- æ€§èƒ½: ä¸å®‰å®š

**å›ºå®šå¾Œï¼ˆEncoderå‡çµï¼‰**:
- è¨“ç·´æ™‚é–“: ç´„40%çŸ­ç¸®
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ç´„30%å‰Šæ¸›
- å®‰å®šæ€§: å®Œå…¨å®‰å®š
- æ€§èƒ½: **å‘ä¸Š**ï¼ˆF1: 0.21 â†’ 0.30ï¼‰

**æ•™è¨“**: 
- äº‹å‰å­¦ç¿’æ¸ˆã¿Encoderã¯ååˆ†ã«å¼·åŠ›
- å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ58,300ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã§ã¯å›ºå®šãŒæœ€é©
- Classifierã®è¨“ç·´ã®ã¿ã§ååˆ†ãªæ€§èƒ½ã‚’é”æˆå¯èƒ½

### 3. æ•°å€¤å®‰å®šæ€§ã¯æœ€å„ªå…ˆ

**å¯¾ç­–ã®éšå±¤**:

1. **ãƒ‡ãƒ¼ã‚¿ãƒ¬ãƒ™ãƒ«**: `np.nan_to_num()`, å®‰å…¨ãªé™¤ç®—
2. **ç‰¹å¾´é‡ãƒ¬ãƒ™ãƒ«**: æœ€çµ‚`isfinite()`ãƒã‚§ãƒƒã‚¯
3. **ãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒ™ãƒ«**: `torch.clamp()`, NaNæ¤œå‡º
4. **æå¤±ãƒ¬ãƒ™ãƒ«**: æ‰‹å‹•BCEã€å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
5. **è¨“ç·´ãƒ¬ãƒ™ãƒ«**: NaN/Infãƒãƒƒãƒã®ã‚¹ã‚­ãƒƒãƒ—

**æ•™è¨“**: å¤šå±¤é˜²å¾¡ãŒå¿…é ˆã€‚1å±¤ã ã‘ã§ã¯ä¸ååˆ†ã€‚

### 4. Focal Lossã¯ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«æœ‰åŠ¹

**è¨­å®š**: gamma=3, alpha=autoï¼ˆç•°å¸¸ç‡ã«å¿œã˜ã¦è‡ªå‹•èª¿æ•´ï¼‰

**åŠ¹æœ**:
| æå¤±é–¢æ•° | F1-Score | Precision | Recall |
|---------|----------|-----------|--------|
| BCE     | 0.21     | 0.12      | 0.51   |
| **Focal Loss** | **0.30** | **0.24** | **0.36** |

**æ•™è¨“**: 
- gamma=3ãŒæœ€é©ï¼ˆ2ã‚„4ã‚ˆã‚Šè‰¯ã„ï¼‰
- ç°¡å˜ãªã‚µãƒ³ãƒ—ãƒ«ï¼ˆé«˜ç¢ºä¿¡åº¦ï¼‰ã®æå¤±ã‚’å¤§å¹…å‰Šæ¸›
- é›£ã—ã„ç•°å¸¸ã‚µãƒ³ãƒ—ãƒ«ã«é›†ä¸­å­¦ç¿’

### 5. éç·šå½¢äºˆæ¸¬ã¯å¿…é ˆ

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**:
- v2.0ï¼ˆç·šå½¢äºˆæ¸¬ï¼‰: ã€Œç²¾åº¦ã¯é«˜ã„ãŒã€ç¾å®Ÿã¨ä¹–é›¢ã—ã¦ã„ã‚‹ã€
- v2.1ï¼ˆéç·šå½¢äºˆæ¸¬ï¼‰: ã€Œéå»ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è‡ªç„¶ã«å»¶é•·ã—ã¦ã„ã‚‹ã€

**æŠ€è¡“çš„ãƒ¡ãƒªãƒƒãƒˆ**:
- 2æ¬¡å¤šé …å¼ã§åŠ é€Ÿåº¦å¤‰åŒ–ã‚’æ‰ãˆã‚‹
- æ¸›è¡°ä¿‚æ•°ã§é•·æœŸäºˆæ¸¬ã®ä¸ç¢ºå®Ÿæ€§ã‚’è¡¨ç¾
- Â±3Ïƒåˆ¶é™ã§ç‰©ç†çš„å¦¥å½“æ€§ã‚’ä¿è¨¼

**æ•™è¨“**: ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç›´æ„Ÿã«åˆã£ãŸäºˆæ¸¬ãŒé‡è¦

### 6. å­¦ç¿’ç‡ã®å½±éŸ¿ã¯å¤§ãã„

**å®Ÿé¨“çµæœ**:

| å­¦ç¿’ç‡ | åæŸé€Ÿåº¦ | æœ€çµ‚F1 | å®‰å®šæ€§ |
|-------|---------|--------|--------|
| 1e-5  | éå¸¸ã«é…ã„ | 0.21 | å®‰å®š |
| 1e-4  | é…ã„ | 0.25 | å®‰å®š |
| **5e-4** | **é©åˆ‡** | **0.30** | **å®‰å®š** |
| 1e-3  | é€Ÿã„ | 0.27 | ä¸å®‰å®š |

**æœ€é©**: lr=5e-4, CosineAnnealingLR

**æ•™è¨“**: Encoderå›ºå®šæ™‚ã¯é«˜ã‚ã®å­¦ç¿’ç‡ãŒæœ‰åŠ¹

### 7. 90æ—¥ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ãŒæœ€ã‚‚äºˆæ¸¬ã—ã‚„ã™ã„

**ä»®èª¬**: é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã¯çŸ­æœŸå¤‰å‹•ã‚ˆã‚Šå®‰å®š

**çµæœ**:
- 30d: F1=0.2903
- 60d: F1=0.2704
- **90d: F1=0.3055** â† Best

**è€ƒå¯Ÿ**:
- çŸ­æœŸï¼ˆ30dï¼‰: ãƒã‚¤ã‚ºã®å½±éŸ¿å¤§
- ä¸­æœŸï¼ˆ60dï¼‰: é·ç§»æœŸã§ä¸å®‰å®š
- **é•·æœŸï¼ˆ90dï¼‰: ãƒˆãƒ¬ãƒ³ãƒ‰ãŒæ˜ç¢º**

**æ•™è¨“**: é•·æœŸäºˆæ¸¬ã§ã¯çµ±è¨ˆçš„å®‰å®šæ€§ãŒæœ‰åˆ©

---

## ğŸ”§ å®Ÿè£…ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

```python
class EnhancedHybridDataset(Dataset):
    def __init__(self, df, feature_cols, horizon):
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
        self.sequences = []
        for seq_str in df['values_sequence'].values:
            values = self._parse_sequence(seq_str)
            values = self._pad_or_trim(values, LOOKBACK_DAYS)
            self.sequences.append(values)
        
        # çµ±è¨ˆç‰¹å¾´é‡ï¼ˆæ•°å€¤å‹ã«å¤‰æ›ï¼‰
        feature_values = df[feature_cols].values
        feature_values = pd.to_numeric(feature_values, errors='coerce')
        self.features = np.nan_to_num(feature_values, nan=0.0)
```

### 2. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰

```python
# 1. Granite TSãƒ¢ãƒ‡ãƒ«ï¼ˆLoRAè¨­å®šä»˜ãï¼‰
lora_config = {"r": 8, "lora_alpha": 16, "lora_dropout": 0.1, "bias": "none"}
granite_model = GraniteTimeSeriesClassifier(
    num_horizons=len(FORECAST_HORIZONS),
    device=device,
    lora_config=lora_config
)

# 2. Encoderã‚’å›ºå®š
for param in granite_model.encoder.parameters():
    param.requires_grad = False
granite_model.encoder.eval()

# 3. Classifierã®ã¿è¨“ç·´å¯èƒ½
model = EnhancedHybridModel(
    granite_model=granite_model,
    stat_feature_dim=28,
    hidden_dim=128,
    dropout=0.3
)
```

### 3. è¨“ç·´ãƒ«ãƒ¼ãƒ—

```python
for epoch in range(epochs):
    model.train()
    for batch in train_loader:
        sequences, features, labels = batch
        
        optimizer.zero_grad()
        outputs = model(sequences, features)
        
        # NaN/Infãƒã‚§ãƒƒã‚¯
        if torch.isnan(outputs).any() or torch.isinf(outputs).any():
            print("Warning: NaN/Inf in outputs, skipping batch")
            continue
        
        loss = criterion(outputs, labels)
        
        if torch.isnan(loss) or torch.isinf(loss):
            print("Warning: NaN/Inf in loss, skipping batch")
            continue
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm=1.0)
        optimizer.step()
    
    scheduler.step()
```

### 4. ãƒ¢ãƒ‡ãƒ«ä¿å­˜

```python
# Best F1-Scoreã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
torch.save({
    'model_state_dict': model.state_dict(),
    'threshold': best_threshold,
    'metrics': {
        'f1': best_f1,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'roc_auc': roc_auc,
        'pr_auc': pr_auc
    },
    'history': {
        'train_loss': train_losses,
        'test_loss': test_losses,
        'test_f1': test_f1s
    }
}, model_path)
```

### 5. å¯è¦–åŒ–

```python
# å¤šæ§˜ãªã‚µãƒ³ãƒ—ãƒ«é¸æŠ
def select_diverse_samples(predictions, labels, num_samples=5):
    tn = (predictions == 0) & (labels == 0)  # True Negative
    tp = (predictions == 1) & (labels == 1)  # True Positive
    fp = (predictions == 1) & (labels == 0)  # False Positive
    fn = (predictions == 0) & (labels == 1)  # False Negative
    
    samples = []
    if tn.any(): samples.append(np.random.choice(np.where(tn)[0]))
    if tp.any(): samples.append(np.random.choice(np.where(tp)[0]))
    if fp.any(): samples.append(np.random.choice(np.where(fp)[0]))
    if fn.any(): samples.append(np.random.choice(np.where(fn)[0]))
    
    while len(samples) < num_samples:
        samples.append(np.random.randint(0, len(predictions)))
    
    return samples
```

---

## ğŸ“ˆ æ€§èƒ½åˆ†æ

### Confusion Matrixï¼ˆ30æ—¥ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼‰

```
                 Predicted
               Normal  Anomaly
Actual Normal   7459    502    (FP rate: 6.3%)
      Anomaly   502     282    (Recall: 36.0%)
```

**åˆ†æ**:
- **True Negative (7459)**: æ­£å¸¸ã‚’æ­£ã—ãäºˆæ¸¬ï¼ˆé«˜ã„ç‰¹ç•°åº¦ï¼‰
- **True Positive (282)**: ç•°å¸¸ã‚’æ­£ã—ãæ¤œå‡º
- **False Positive (502)**: éæ¤œçŸ¥ï¼ˆè¨±å®¹ç¯„å›²ï¼‰
- **False Negative (502)**: è¦‹é€ƒã—ï¼ˆæ”¹å–„ä½™åœ°ã‚ã‚Šï¼‰

### ã‚¨ãƒ©ãƒ¼åˆ†æ

**False Negativeï¼ˆè¦‹é€ƒã—ï¼‰ã®ç‰¹å¾´**:
1. ç•°å¸¸ã®åˆæœŸæ®µéšï¼ˆå¾®å°ãªå¤‰åŒ–ï¼‰
2. å­£ç¯€å¤‰å‹•ã¨ã®æ··åŒ
3. ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤šæ§˜æ€§

**False Positiveï¼ˆéæ¤œçŸ¥ï¼‰ã®ç‰¹å¾´**:
1. æ€¥æ¿€ã ãŒä¸€æ™‚çš„ãªå¤‰å‹•
2. ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æœŸé–“ã®å½±éŸ¿
3. ã‚»ãƒ³ã‚µãƒ¼ãƒã‚¤ã‚º

**æ”¹å–„æ¡ˆ**:
- æ™‚é–“çš„æ–‡è„ˆã®æ´»ç”¨ï¼ˆé€£ç¶šã—ãŸç•°å¸¸ã®é‡ã¿ä»˜ã‘ï¼‰
- å¤–éƒ¨æƒ…å ±ã®çµ±åˆï¼ˆãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨˜éŒ²ï¼‰
- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼ˆè¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æŠ•ç¥¨ï¼‰

---

## ğŸš€ ä»Šå¾Œã®å±•é–‹ï¼ˆv2.2ä»¥é™ï¼‰

### çŸ­æœŸæ”¹å–„ï¼ˆv2.2ï¼‰

1. **Attentionæ©Ÿæ§‹ã®å°å…¥**
   - éå»90æ—¥ã®é‡è¦ãªæ™‚ç‚¹ã‚’è‡ªå‹•é¸æŠ
   - å­£ç¯€æ€§ãƒ»å‘¨æœŸæ€§ã®æ˜ç¤ºçš„ãƒ¢ãƒ‡ãƒ«åŒ–

2. **Multi-Task Learning**
   - 3ã¤ã®ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã‚’åŒæ™‚å­¦ç¿’
   - å…±é€šç‰¹å¾´ã®åŠ¹ç‡çš„æŠ½å‡º
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å‰Šæ¸›

3. **Data Augmentation**
   - æ™‚ç³»åˆ—ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
   - ãƒã‚¤ã‚ºæ³¨å…¥
   - è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«å¢—å¼·

### ä¸­æœŸæ”¹å–„ï¼ˆv2.3ï¼‰

4. **Uncertainty Quantification**
   - äºˆæ¸¬ã®ä¿¡é ¼åŒºé–“
   - Monte Carlo Dropout
   - ãƒ™ã‚¤ã‚ºçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

5. **Online Learning**
   - æ–°ãƒ‡ãƒ¼ã‚¿ã§ã®ç¶™ç¶šå­¦ç¿’
   - Concept Driftã¸ã®å¯¾å¿œ
   - é©å¿œçš„é–¾å€¤èª¿æ•´

6. **èª¬æ˜å¯èƒ½æ€§ã®å‘ä¸Š**
   - SHAP values
   - Attention weightså¯è¦–åŒ–
   - ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ

### é•·æœŸå±•æœ›ï¼ˆv3.0ï¼‰

7. **Transformerå®Œå…¨ç§»è¡Œ**
   - TinyTimeMixerã‹ã‚‰æœ€æ–°Transformerã¸
   - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒï¼‰

8. **å¼·åŒ–å­¦ç¿’ã®çµ±åˆ**
   - ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹è¨ˆç”»ã®æœ€é©åŒ–
   - ã‚³ã‚¹ãƒˆé–¢æ•°ã®å­¦ç¿’

9. **å¤§è¦æ¨¡å±•é–‹**
   - è¤‡æ•°æ–½è¨­ã¸ã®æ‹¡å¼µ
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–
   - ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œ

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Focal Loss for Dense Object Detection** (Lin et al., 2017)
   - https://arxiv.org/abs/1708.02002
   - Focal Lossã®ç†è«–çš„åŸºç¤

2. **LoRA: Low-Rank Adaptation of Large Language Models** (Hu et al., 2021)
   - https://arxiv.org/abs/2106.09685
   - LoRAã®åŠ¹ç‡çš„é©ç”¨

3. **TinyTimeMixer** (IBM Research)
   - Lightweight time series forecasting
   - äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ´»ç”¨

4. **LightGBM: A Highly Efficient Gradient Boosting Decision Tree** (Ke et al., 2017)
   - https://papers.nips.cc/paper/6907-lightgbm
   - ç‰¹å¾´é‡çµ±åˆã®å‚è€ƒ

---

## ğŸ’¾ å†ç¾æ€§ã®ãŸã‚ã®æƒ…å ±

### ç’°å¢ƒ

- Python: 3.12
- PyTorch: 2.6+
- NumPy: <2.0ï¼ˆPyTorchäº’æ›æ€§ï¼‰
- Transformers: æœ€æ–°
- PEFT: æœ€æ–°ï¼ˆLoRAç”¨ï¼‰

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
# ãƒ¢ãƒ‡ãƒ«
d_model = 64  # TinyTimeMixer embedding dimension
stat_features = 28
hidden_dim = 128
dropout = 0.3

# LoRA
lora_r = 8
lora_alpha = 16
lora_dropout = 0.1

# è¨“ç·´
epochs = 20
batch_size = 128
learning_rate = 5e-4
weight_decay = 0.01
max_grad_norm = 1.0

# Focal Loss
gamma = 3.0
alpha = auto  # ç•°å¸¸ç‡ã«å¿œã˜ã¦è‡ªå‹•è¨ˆç®—

# éç·šå½¢äºˆæ¸¬
polynomial_degree = 2
decay_factor = exp(-x / (horizon * 2))
clip_range = mean Â± 3*std
```

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

- è¨“ç·´: 58,300ã‚µãƒ³ãƒ—ãƒ«
- ãƒ†ã‚¹ãƒˆ: 8,745ã‚µãƒ³ãƒ—ãƒ«
- ç•°å¸¸ç‡: ç´„9%ï¼ˆå…¨ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼‰
- Lookback: 90æ—¥
- ãƒ›ãƒ©ã‚¤ã‚ºãƒ³: 30æ—¥, 60æ—¥, 90æ—¥

### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
models/hybrid_model_v2.1/
â”œâ”€â”€ pytorch_model_30d.pt  # 30æ—¥ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ pytorch_model_60d.pt  # 60æ—¥ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ pytorch_model_90d.pt  # 90æ—¥ãƒ¢ãƒ‡ãƒ«
â””â”€â”€ metrics_summary_v2.1.csv

results/
â”œâ”€â”€ forecast_comparison_v2.1_20260215_190834.png
â””â”€â”€ training_history_v2.1_*.png (ä»Šå¾Œç”Ÿæˆ)
```

---

## ğŸ¯ çµè«–

### æˆåŠŸè¦å› 

1. **Encoderå›ºå®šæˆ¦ç•¥**: è¨ˆç®—åŠ¹ç‡ã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹
2. **åŒ…æ‹¬çš„ãªNaN/Infå¯¾ç­–**: å¤šå±¤é˜²å¾¡ã§å®Œå…¨å®‰å®šåŒ–
3. **Focal Loss**: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã¸ã®åŠ¹æœçš„å¯¾å¿œ
4. **çµ±è¨ˆç‰¹å¾´é‡**: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ´»ç”¨
5. **éç·šå½¢äºˆæ¸¬**: ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã¸ã®å¯¾å¿œ

### æ®‹ã•ã‚ŒãŸèª²é¡Œ

1. **Recallå‘ä¸Š**: è¦‹é€ƒã—å‰Šæ¸›ï¼ˆç¾åœ¨36%ï¼‰
2. **é•·æœŸå®‰å®šæ€§**: 90æ—¥ä»¥ä¸Šã®äºˆæ¸¬
3. **èª¬æ˜å¯èƒ½æ€§**: ãªãœç•°å¸¸ã¨åˆ¤æ–­ã—ãŸã‹
4. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§**: æ¨è«–é€Ÿåº¦ã®æ”¹å–„
5. **æ±åŒ–æ€§èƒ½**: æ–°è¦è¨­å‚™ã¸ã®è»¢ç§»

### æœ€çµ‚è©•ä¾¡

v2.1ã¯ã€v2.0ã®èª²é¡Œã‚’**ã™ã¹ã¦è§£æ±º**ã—ã€ä»¥ä¸‹ã‚’é”æˆï¼š
- âœ… F1-Score 40%æ”¹å–„
- âœ… NaN/Infå•é¡Œå®Œå…¨è§£æ±º
- âœ… éç·šå½¢äºˆæ¸¬ã®å®Ÿç¾
- âœ… è¨“ç·´ã®é«˜é€ŸåŒ–ãƒ»å®‰å®šåŒ–
- âœ… è¨­å‚™ç®¡ç†è€…ã®ç†è§£æ€§å‘ä¸Š

**v2.1ã¯æœ¬ç•ªé‹ç”¨å¯èƒ½ãªãƒ¬ãƒ™ãƒ«ã«åˆ°é”**

---

**Document Version**: 1.0  
**Last Updated**: 2026-02-15  
**Author**: HVAC Anomaly Detection Team  
**Status**: Production Ready ğŸš€
