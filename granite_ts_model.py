"""
Granite Time Series Model with LoRA
Granite TSãƒ¢ãƒ‡ãƒ«å®Ÿè£…ï¼ˆLoRAçµ±åˆï¼‰

æ©Ÿèƒ½:
1. Granite Time Series Foundation Modelã®ãƒ­ãƒ¼ãƒ‰
2. LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨
3. ãƒã‚¤ãƒŠãƒªåˆ†é¡ãƒ˜ãƒƒãƒ‰ã®è¿½åŠ 
4. ãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³äºˆæ¸¬ï¼ˆ30æ—¥ã€60æ—¥ã€90æ—¥ï¼‰
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple
import numpy as np

# å„ä¾å­˜é–¢ä¿‚ã‚’å€‹åˆ¥ã«ãƒ†ã‚¹ãƒˆ
GRANITE_TS_AVAILABLE = True
import_errors = []

try:
    from transformers import AutoModel, AutoConfig
except ImportError as e:
    import_errors.append(f"transformers: {e}")
    GRANITE_TS_AVAILABLE = False

try:
    from peft import (
        LoraConfig,
        get_peft_model,
        TaskType,
        PeftModel
    )
except ImportError as e:
    import_errors.append(f"peft: {e}")
    GRANITE_TS_AVAILABLE = False

try:
    # Granite TS specific imports
    from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction
    from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor
except ImportError as e:
    import_errors.append(f"tsfm_public: {e}")
    GRANITE_TS_AVAILABLE = False

if not GRANITE_TS_AVAILABLE:
    print(f"[WARNING] Some dependencies not available:")
    for err in import_errors:
        print(f"  - {err}")
    print("Run: pip install transformers peft")
    print("For Granite TS: pip install git+https://github.com/ibm-granite/granite-tsfm.git")

from config import (
    GRANITE_MODEL_NAME,
    CONTEXT_LENGTH,
    LORA_CONFIG,
    FORECAST_HORIZONS,
    LOOKBACK_DAYS,
    USE_GPU,
    GPU_ID
)


class GraniteTimeSeriesClassifier(nn.Module):
    """
    Granite Time Series + LoRA ã«ã‚ˆã‚‹é€¸è„±äºˆæ¸¬åˆ†é¡å™¨
    """
    
    def __init__(
        self,
        model_name: str = GRANITE_MODEL_NAME,
        num_horizons: int = len(FORECAST_HORIZONS),
        lora_config: Optional[Dict] = None,
        device: Optional[str] = None
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name: Granite TSãƒ¢ãƒ‡ãƒ«å
            num_horizons: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³æ•°
            lora_config: LoRAè¨­å®š
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
        """
        super().__init__()
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if device is None:
            if USE_GPU and torch.cuda.is_available():
                self.device = torch.device(f'cuda:{GPU_ID}')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = torch.device(device)
        
        print(f"Using device: {self.device}")
        
        # LoRAè¨­å®š
        if lora_config is None:
            lora_config = LORA_CONFIG
        
        self.num_horizons = num_horizons
        self.model_name = model_name
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        self._build_model(lora_config)
        
    def _build_model(self, lora_config: Dict):
        """
        ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        
        Args:
            lora_config: LoRAè¨­å®š
        """
        print(f"Building Granite TS model: {self.model_name}")
        
        # Granite TSãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if not GRANITE_TS_AVAILABLE:
            print("  [WARNING] Granite TS not available, using fallback LSTM model...")
            self._build_fallback_model()
            return
        
        try:
            # Granite TS TinyTimeMixerãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            print("  Loading Granite TS TinyTimeMixer model...")
            
            from tsfm_public.models.tinytimemixer import TinyTimeMixerConfig
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®š
            config = TinyTimeMixerConfig(
                context_length=LOOKBACK_DAYS,
                prediction_length=max(FORECAST_HORIZONS),
                num_input_channels=1,  # å˜å¤‰é‡æ™‚ç³»åˆ—
                decoder_mode='flatten',  # åˆ†é¡ã‚¿ã‚¹ã‚¯ç”¨
                d_model=64,
                num_layers=4,
                dropout=0.1,
            )
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            self.base_model = TinyTimeMixerForPrediction(config)
            self.hidden_size = config.d_model
            
            print(f"  [OK] Granite TS TinyTimeMixer loaded (d_model={self.hidden_size})")
            
            # LoRAé©ç”¨
            print("  Applying LoRA...")
            
            # TinyTimeMixerã®å…·ä½“çš„ãªLinearå±¤ã‚’å¯¾è±¡ã«ã™ã‚‹
            target_modules = [
                "encoder.patcher",  # ãƒ‘ãƒƒãƒãƒ£ãƒ¼å±¤
                "mlp.fc1",  # MLPã®ç¬¬1å±¤ï¼ˆã™ã¹ã¦ã®MLPå†…ï¼‰
                "mlp.fc2",  # MLPã®ç¬¬2å±¤ï¼ˆã™ã¹ã¦ã®MLPå†…ï¼‰
                "attn_layer",  # ã‚²ãƒ¼ãƒˆä»˜ãæ³¨æ„æ©Ÿæ§‹å±¤
            ]
            print(f"  Target modules for LoRA: {target_modules}")
            
            peft_config = LoraConfig(
                r=lora_config.get("r", 8),
                lora_alpha=lora_config.get("lora_alpha", 16),
                target_modules=target_modules,
                lora_dropout=lora_config.get("lora_dropout", 0.1),
                bias=lora_config.get("bias", "none"),
            )
            
            self.model = get_peft_model(self.base_model, peft_config)
            
            # å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º
            self.model.print_trainable_parameters()
            
        except Exception as e:
            print(f"  [WARNING] Could not load Granite TS model")
            print(f"  Error type: {type(e).__name__}")
            print(f"  Error message: {str(e)}")
            import traceback
            print(f"  Traceback:")
            traceback.print_exc()
            print("  Using fallback LSTM model for MVP testing...")
            self._build_fallback_model()
            return
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰è¿½åŠ 
        self._add_classification_heads()
    
    def _build_fallback_model(self):
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ï¼ˆLSTMï¼‰
        Granite TSãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªLSTMãƒ¢ãƒ‡ãƒ«
        """
        self.hidden_size = 128
        self.num_layers = 2
        
        # LSTMå±¤
        self.lstm = nn.LSTM(
            input_size=1,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            batch_first=True,
            dropout=0.3
        )
        
        # æ­£è¦åŒ–å±¤
        self.layer_norm = nn.LayerNorm(self.hidden_size)
        
        # Attentionå±¤ï¼ˆç°¡æ˜“ï¼‰
        self.attention = nn.MultiheadAttention(
            embed_dim=self.hidden_size,
            num_heads=4,
            batch_first=True
        )
        
        print(f"[OK] Fallback LSTM model built (hidden={self.hidden_size}, layers={self.num_layers})")
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚‚è¿½åŠ 
        self._add_classification_heads()
    
    def _add_classification_heads(self):
        """åˆ†é¡ãƒ˜ãƒƒãƒ‰è¿½åŠ ï¼ˆãƒãƒ«ãƒãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼‰"""
        
        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›æ¬¡å…ƒã‚’å–å¾—
        try:
            hidden_size = self.config.hidden_size
        except:
            hidden_size = self.hidden_size
        
        # å„ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã«å¯¾ã™ã‚‹åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.classification_heads = nn.ModuleDict({
            f"head_{h}d": nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_size // 2, hidden_size // 4),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_size // 4, 1),  # ãƒã‚¤ãƒŠãƒªåˆ†é¡
                nn.Sigmoid()
            )
            for h in FORECAST_HORIZONS
        })
        
        print(f"[OK] Classification heads added for horizons: {FORECAST_HORIZONS}")
    
    def forward(
        self,
        input_sequence: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        é †ä¼æ’­
        
        Args:
            input_sequence: å…¥åŠ›ç³»åˆ— [batch_size, seq_len, 1]
            attention_mask: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ [batch_size, seq_len]
            
        Returns:
            å„ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®äºˆæ¸¬ç¢ºç‡ã®è¾æ›¸
        """
        batch_size = input_sequence.size(0)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒGranite TSã‹LSTMã‹ã§å‡¦ç†ã‚’åˆ†å²
        if hasattr(self, 'lstm'):
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯LSTMãƒ¢ãƒ‡ãƒ«
            lstm_out, (hidden, cell) = self.lstm(input_sequence)
            
            # æœ€çµ‚æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
            last_output = lstm_out[:, -1, :]  # [batch_size, hidden_size]
            
            # æ­£è¦åŒ–
            normalized_output = self.layer_norm(last_output)
            
            # Attentionï¼ˆã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰
            attn_output, _ = self.attention(
                lstm_out,
                lstm_out,
                lstm_out
            )
            pooled_output = attn_output[:, -1, :]  # æœ€çµ‚æ™‚åˆ»
            
        elif hasattr(self, 'model') and hasattr(self.model, 'base_model'):
            # Granite TS TinyTimeMixerãƒ¢ãƒ‡ãƒ«
            # å…¥åŠ›å½¢çŠ¶ã‚’èª¿æ•´: [batch_size, seq_len, num_channels]
            if input_sequence.dim() == 2:
                input_sequence = input_sequence.unsqueeze(-1)
            
            # TinyTimeMixerã§äºˆæ¸¬ã‚’ç”Ÿæˆ
            outputs = self.model(
                past_values=input_sequence,
                return_dict=True
            )
            
            # äºˆæ¸¬å€¤ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨
            # TinyTimeMixerã®å‡ºåŠ›: [batch_size, prediction_length, num_channels]
            predictions = outputs.prediction_outputs if hasattr(outputs, 'prediction_outputs') else outputs[0]
            # print(f"DEBUG: predictions shape = {predictions.shape}")
            
            # æ™‚é–“è»¸ã¨ãƒãƒ£ãƒãƒ«è»¸ã‚’å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã—ã¦ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
            # [batch_size, pred_len, channels] -> [batch_size, pred_len*channels]
            batch_size = predictions.size(0)
            pooled_output = predictions.reshape(batch_size, -1)  # ãƒ•ãƒ©ãƒƒãƒˆåŒ–
            # print(f"DEBUG: pooled_output shape = {pooled_output.shape}")
            
            # ç·šå½¢å±¤ã§æ¬¡å…ƒã‚’èª¿æ•´ï¼ˆä¸€åº¦ã ã‘åˆæœŸåŒ–ï¼‰
            if not hasattr(self, 'feature_proj'):
                feature_dim = pooled_output.size(-1)
                self.feature_proj = nn.Linear(feature_dim, self.hidden_size).to(self.device)
                # print(f"DEBUG: Created feature_proj: {feature_dim} -> {self.hidden_size}")
            
            pooled_output = self.feature_proj(pooled_output)
            
        else:
            # ä¸€èˆ¬çš„ãªTransformerãƒ¢ãƒ‡ãƒ«
            outputs = self.model(
                inputs_embeds=input_sequence,
                attention_mask=attention_mask,
                return_dict=True
            )
            
            # ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆæœ€çµ‚éš ã‚ŒçŠ¶æ…‹ï¼‰
            last_hidden_state = outputs.last_hidden_state
            pooled_output = last_hidden_state[:, -1, :]  # [batch_size, hidden_size]
        
        # å„ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®äºˆæ¸¬
        predictions = {}
        for h in FORECAST_HORIZONS:
            head_name = f"head_{h}d"
            pred = self.classification_heads[head_name](pooled_output)
            predictions[f"prob_{h}d"] = pred.squeeze(-1)  # [batch_size]
        
        return predictions
    
    def predict(
        self,
        input_sequence: np.ndarray,
        return_probs: bool = True
    ) -> Dict[str, np.ndarray]:
        """
        æ¨è«–
        
        Args:
            input_sequence: å…¥åŠ›ç³»åˆ— [batch_size, seq_len] or [seq_len]
            return_probs: ç¢ºç‡ã‚’è¿”ã™ã‹ï¼ˆFalseã®å ´åˆã¯ãƒã‚¤ãƒŠãƒªãƒ©ãƒ™ãƒ«ï¼‰
            
        Returns:
            äºˆæ¸¬çµæœã®è¾æ›¸
        """
        self.eval()
        
        # æ¬¡å…ƒèª¿æ•´
        if input_sequence.ndim == 1:
            input_sequence = input_sequence[np.newaxis, :, np.newaxis]  # [1, seq_len, 1]
        elif input_sequence.ndim == 2:
            input_sequence = input_sequence[:, :, np.newaxis]  # [batch_size, seq_len, 1]
        
        # Tensorã«å¤‰æ›
        input_tensor = torch.FloatTensor(input_sequence).to(self.device)
        
        # æ¨è«–
        with torch.no_grad():
            predictions = self.forward(input_tensor)
        
        # CPUãƒ»NumPyã«å¤‰æ›
        results = {}
        for key, value in predictions.items():
            probs = value.cpu().numpy()
            
            if return_probs:
                results[key] = probs
            else:
                # 0.5ã‚’é–¾å€¤ã«ãƒã‚¤ãƒŠãƒªåŒ–
                results[key.replace('prob', 'label')] = (probs > 0.5).astype(int)
        
        return results
    
    def save_model(self, save_path: str):
        """
        ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        
        Args:
            save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        """
        print(f"Saving model to: {save_path}")
        
        # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ä¿å­˜ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯ä¿å­˜ã—ãªã„ï¼‰
        if hasattr(self, 'model') and isinstance(self.model, PeftModel):
            self.model.save_pretrained(save_path)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯å…¨ä½“ã‚’ä¿å­˜
            torch.save(self.state_dict(), f"{save_path}/model.pt")
        
        print("[OK] Model saved successfully")
    
    def load_model(self, load_path: str):
        """
        ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        
        Args:
            load_path: èª­ã¿è¾¼ã¿å…ƒãƒ‘ã‚¹
        """
        print(f"Loading model from: {load_path}")
        
        try:
            # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼èª­ã¿è¾¼ã¿
            self.model = PeftModel.from_pretrained(
                self.base_model,
                load_path
            )
            print("[OK] LoRA adapter loaded")
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            state_dict = torch.load(f"{load_path}/model.pt", map_location=self.device)
            self.load_state_dict(state_dict)
            print("[OK] Model loaded")
        
        self.to(self.device)


def test_model():
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
    print("="*60)
    print("ğŸ§ª Testing Granite TS Classifier")
    print("="*60)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = GraniteTimeSeriesClassifier()
    model.to(model.device)
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    batch_size = 4
    seq_len = 90
    
    dummy_input = torch.randn(batch_size, seq_len, 1).to(model.device)
    
    print(f"\nğŸ“Š Testing with input shape: {dummy_input.shape}")
    
    # é †ä¼æ’­ãƒ†ã‚¹ãƒˆ
    predictions = model(dummy_input)
    
    print("\n[OK] Forward pass successful")
    print("Predictions:")
    for key, value in predictions.items():
        print(f"  {key}: {value.shape} -> {value[0].item():.4f}")
    
    # NumPyå…¥åŠ›ã§ã®ãƒ†ã‚¹ãƒˆ
    numpy_input = np.random.randn(seq_len)
    results = model.predict(numpy_input)
    
    print("\n[OK] Prediction successful")
    print("Results:")
    for key, value in results.items():
        print(f"  {key}: {value}")
    
    print("\n" + "="*60)
    print("[OK] Model test complete!")
    print("="*60)


if __name__ == "__main__":
    test_model()
