"""
Hybrid Model v2.1: Enhanced Time Series Forecasting
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«v2.1 - éç·šå½¢äºˆæ¸¬ã¨é«˜åº¦ãªç‰¹å¾´æ´»ç”¨

æ”¹å–„ç‚¹:
1. Statistical Features (28æ¬¡å…ƒ) ã‚’å¤šæ¬¡å…ƒã‚¤ãƒ³ãƒ—ãƒƒãƒˆ
   - TinyTimeMixerã¨Classifierä¸¡æ–¹ã§æ´»ç”¨
   - åŸ‹ã‚è¾¼ã¿å¾Œã«çµ±è¨ˆç‰¹å¾´ã‚’çµåˆ

2. TinyTimeMixer Encoderï¼ˆäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ»å›ºå®šï¼‰
   - äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’æ´»ç”¨
   - Encoderã¯å›ºå®šã—ã€è¨“ç·´æ™‚é–“ã‚’çŸ­ç¸®
   - Feature Fusion + Classifierã®ã¿è¨“ç·´

3. Focal Loss (beta=3)
   - ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œ
   - é›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã«é‡ç‚¹çš„ã«å­¦ç¿’

4. éç·šå½¢äºˆæ¸¬
   - éå»90æ—¥ã®å»¶é•·ç·šä¸Šã«ã‚ã‚‹äºˆæ¸¬
   - è¨­å‚™ç®¡ç†è€…ã«ã¨ã£ã¦ç›´æ„Ÿçš„ãªå¯è¦–åŒ–
"""

import sys
import os

# Granite TSç”¨ã®å›é¿ç­–
sys.modules['torchvision'] = None
os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

import lightgbm as lgb
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    precision_score,
    recall_score,
    f1_score,
    accuracy_score,
    precision_recall_curve
)
import matplotlib.pyplot as plt
import seaborn as sns

from config import (
    PROCESSED_DATA_DIR,
    MODEL_ROOT,
    RESULTS_ROOT,
    FORECAST_HORIZONS,
    RANDOM_SEED,
    LOOKBACK_DAYS,
    USE_GPU,
    GPU_ID
)

from granite_ts_model import GraniteTimeSeriesClassifier

# ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")


class FocalLoss(nn.Module):
    """
    Focal Loss for handling class imbalance
    
    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)
    
    Args:
        alpha: ãƒãƒ©ãƒ³ã‚¹ä¿‚æ•°ï¼ˆpositive classã®é‡ã¿ï¼‰
        gamma: focusing parameterï¼ˆé›£ã—ã„ã‚µãƒ³ãƒ—ãƒ«ã¸ã®é‡ç‚¹åº¦ï¼‰
    """
    
    def __init__(self, alpha: float = 0.25, gamma: float = 3.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Args:
            inputs: äºˆæ¸¬ç¢ºç‡ [batch_size]
            targets: ãƒ©ãƒ™ãƒ« [batch_size]
        
        Returns:
            loss: ã‚¹ã‚«ãƒ©ãƒ¼æå¤±å€¤
        """
        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã«ã‚¯ãƒªãƒƒãƒ—ã¨å‹å¤‰æ›
        eps = 1e-7
        inputs = torch.clamp(inputs, eps, 1 - eps)
        targets = targets.float()  # targetsã‚’ç¢ºå®Ÿã«floatã«
        
        # NaNã¨Infã®ãƒã‚§ãƒƒã‚¯
        if torch.isnan(inputs).any() or torch.isinf(inputs).any():
            print(f"Warning: inputs contains NaN or Inf")
            inputs = torch.nan_to_num(inputs, nan=0.5, posinf=1-eps, neginf=eps)
        
        # Binary Cross Entropyï¼ˆæ‰‹å‹•è¨ˆç®—ã§ã‚ˆã‚Šå®‰å®šï¼‰
        bce_loss = -(targets * torch.log(inputs) + (1 - targets) * torch.log(1 - inputs))
        
        # p_t: æ­£è§£ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡
        p_t = inputs * targets + (1 - inputs) * (1 - targets)
        p_t = torch.clamp(p_t, eps, 1 - eps)  # ã•ã‚‰ã«ã‚¯ãƒªãƒƒãƒ—
        
        # Focal term: (1 - p_t)^gamma
        focal_weight = (1 - p_t) ** self.gamma
        
        # Alpha balancing
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        
        # Focal Loss
        focal_loss = alpha_t * focal_weight * bce_loss
        
        return focal_loss.mean()


class EnhancedHybridDataset(Dataset):
    """
    v2.1 Enhanced Dataset
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ + çµ±è¨ˆç‰¹å¾´é‡ã‚’ä¸¡æ–¹è¿”ã™
    """
    
    def __init__(self, df: pd.DataFrame, feature_cols: List[str], horizon: int):
        self.df = df
        self.feature_cols = feature_cols
        self.horizon = horizon
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®è§£æ
        self.sequences = []
        for seq_str in df['values_sequence'].values:
            import ast
            try:
                values = ast.literal_eval(seq_str)
            except:
                values = [float(x.strip('[] ')) for x in seq_str.split(',') if x.strip()]
            
            # LOOKBACK_DAYSæ—¥åˆ†ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/ãƒˆãƒªãƒŸãƒ³ã‚°
            if len(values) < LOOKBACK_DAYS:
                values = [values[0]] * (LOOKBACK_DAYS - len(values)) + values
            elif len(values) > LOOKBACK_DAYS:
                values = values[-LOOKBACK_DAYS:]
            self.sequences.append(values)
        
        # çµ±è¨ˆç‰¹å¾´é‡
        self.features = df[feature_cols].values.astype(np.float32)
        
        # ãƒ©ãƒ™ãƒ«
        label_col = f'label_{horizon}d'
        self.labels = df[label_col].values.astype(np.float32)
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        # æ™‚ç³»åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ [seq_len, 1]
        sequence = torch.FloatTensor(self.sequences[idx]).unsqueeze(-1)
        
        # çµ±è¨ˆç‰¹å¾´é‡ [num_features]
        features = torch.FloatTensor(self.features[idx])
        
        # ãƒ©ãƒ™ãƒ«
        label = torch.FloatTensor([self.labels[idx]])
        
        return {
            'sequence': sequence,
            'features': features,
            'label': label
        }


class EnhancedHybridModel(nn.Module):
    """
    v2.1 Enhanced Hybrid Model
    
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:
    1. TinyTimeMixer Encoder (äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ»å›ºå®š)
       - Input: æ™‚ç³»åˆ— [batch, 90, 1]
       - Output: åŸ‹ã‚è¾¼ã¿ [batch, 64]
       - è¨“ç·´ä¸­ã¯å›ºå®šï¼ˆno_gradï¼‰
    
    2. Feature Fusion Layer
       - åŸ‹ã‚è¾¼ã¿ [64] + çµ±è¨ˆç‰¹å¾´ [28] = çµåˆç‰¹å¾´ [92]
    
    3. Multi-Layer Classifierï¼ˆè¨“ç·´å¯èƒ½ï¼‰
       - éç·šå½¢å¤‰æ› â†’ ç•°å¸¸ç¢ºç‡
    """
    
    def __init__(
        self,
        granite_model: GraniteTimeSeriesClassifier,
        stat_feature_dim: int = 28,
        hidden_dim: int = 128,
        dropout: float = 0.3
    ):
        super().__init__()
        
        # TinyTimeMixer Encoder (å›ºå®šï¼šè¨“ç·´ã—ãªã„ï¼‰
        if hasattr(granite_model, 'base_model'):
            self.encoder = granite_model.base_model
        elif hasattr(granite_model, 'model'):
            self.encoder = granite_model.model.base_model
        elif hasattr(granite_model, 'lstm'):
            self.encoder = granite_model.lstm
        else:
            raise ValueError("Could not extract encoder")
        
        # Encoderã‚’å®Œå…¨ã«å›ºå®šï¼ˆLoRAã‚‚å«ã‚€ï¼‰
        for param in self.encoder.parameters():
            param.requires_grad = False
        
        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
        self.encoder.eval()
        
        # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
        self.embedding_dim = 64  # TinyTimeMixer d_model
        
        # Feature Fusion + Classification Head
        self.fusion_layer = nn.Sequential(
            nn.Linear(self.embedding_dim + stat_feature_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, sequence: torch.Tensor, features: torch.Tensor) -> torch.Tensor:
        """
        é †ä¼æ’­
        
        Args:
            sequence: æ™‚ç³»åˆ— [batch, seq_len, 1]
            features: çµ±è¨ˆç‰¹å¾´ [batch, stat_dim]
        
        Returns:
            predictions: ç•°å¸¸ç¢ºç‡ [batch, 1]
        """
        # TinyTimeMixer Embeddingsï¼ˆå›ºå®šãƒ¢ãƒ¼ãƒ‰ï¼‰
        with torch.no_grad():
            outputs = self.encoder(
                past_values=sequence,
                output_hidden_states=True,
                return_dict=True
            )
            
            if hasattr(outputs, 'backbone_hidden_state') and outputs.backbone_hidden_state is not None:
                embeddings = outputs.backbone_hidden_state.squeeze(1).mean(dim=1)  # [batch, 64]
            elif hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                embeddings = outputs.hidden_states[-1].squeeze(1).mean(dim=1)
            else:
                embeddings = torch.mean(sequence, dim=1).squeeze()
                if len(embeddings.shape) == 1:
                    embeddings = embeddings.unsqueeze(-1)
        
        # Embeddingså®‰å®šåŒ–ï¼ˆNaNãƒã‚§ãƒƒã‚¯ï¼‰
        if torch.isnan(embeddings).any() or torch.isinf(embeddings).any():
            print(f"Warning: embeddings contain NaN/Inf, replacing with zeros")
            embeddings = torch.nan_to_num(embeddings, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # çµ±è¨ˆç‰¹å¾´ã®å®‰å®šåŒ–
        if torch.isnan(features).any() or torch.isinf(features).any():
            print(f"Warning: features contain NaN/Inf, replacing with zeros")
            features = torch.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # Feature Fusion
        fused_features = torch.cat([embeddings, features], dim=1)  # [batch, 92]
        
        # Classification
        predictions = self.fusion_layer(fused_features)  # [batch, 1]
        
        # æœ€çµ‚å‡ºåŠ›ã®å®‰å®šåŒ–
        predictions = predictions.squeeze(1)
        predictions = torch.clamp(predictions, 1e-7, 1 - 1e-7)
        
        return predictions


class HybridTrainerV2_1:
    """v2.1 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(self):
        self.device = torch.device(f'cuda:{GPU_ID}' if USE_GPU and torch.cuda.is_available() else 'cpu')
        self.train_df = None
        self.test_df = None
        self.feature_cols = []
        self.models = {}
        self.results = {}
        
        print(f"ğŸ–¥ï¸  Device: {self.device}")
        print(f"ğŸ“ Data directory: {PROCESSED_DATA_DIR}")
        print(f"ğŸ“ Model directory: {MODEL_ROOT}")
    
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰"""
        print("\nğŸ“‚ Loading enriched data...")
        
        train_path = PROCESSED_DATA_DIR / "training_samples_enriched.csv"
        test_path = PROCESSED_DATA_DIR / "test_samples_enriched.csv"
        
        if not train_path.exists() or not test_path.exists():
            raise FileNotFoundError(
                "Enriched data not found. Please run create_enriched_features.py first."
            )
        
        self.train_df = pd.read_csv(train_path)
        self.test_df = pd.read_csv(test_path)
        
        print(f"âœ“ Train: {len(self.train_df):,} samples")
        print(f"âœ“ Test: {len(self.test_df):,} samples")
        
        # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã®ç‰¹å®š
        exclude_cols = [
            'equipment_id', 'check_item_id', 'date', 
            'window_start', 'window_end', 'values_sequence',
            'reference_datetime', 'horizon_datetime',
            'label_current', 'label_30d', 'label_60d', 'label_90d',
            'any_anomaly'
        ]
        
        self.feature_cols = [col for col in self.train_df.columns 
                            if col not in exclude_cols]
        
        print(f"âœ“ Statistical features: {len(self.feature_cols)}d")
    
    def train_horizon(self, horizon: int, epochs: int = 10, batch_size: int = 128, lr: float = 1e-4):
        """
        ç‰¹å®šãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã®ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        
        Args:
            horizon: äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆ30, 60, 90ï¼‰
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            lr: å­¦ç¿’ç‡
        """
        print(f"\n{'='*70}")
        print(f"Training Enhanced Hybrid Model v2.1 for {horizon}d horizon")
        print('='*70)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_dataset = EnhancedHybridDataset(self.train_df, self.feature_cols, horizon)
        test_dataset = EnhancedHybridDataset(self.test_df, self.feature_cols, horizon)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
        
        print(f"\nDataset statistics:")
        print(f"  Train samples: {len(train_dataset):,}")
        print(f"  Test samples: {len(test_dataset):,}")
        print(f"  Train positives: {train_dataset.labels.sum():.0f} ({train_dataset.labels.mean()*100:.1f}%)")
        print(f"  Test positives: {test_dataset.labels.sum():.0f} ({test_dataset.labels.mean()*100:.1f}%)")
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        print(f"\nğŸ—ï¸  Building Enhanced Hybrid Model...")
        
        # LoRAè¨­å®šï¼ˆr=8, alpha=16ã§è¦æ¨¡ã‚’åˆ¶å¾¡ï¼‰
        lora_config = {
            "r": 8,
            "lora_alpha": 16,
            "lora_dropout": 0.1,
            "bias": "none"
        }
        
        granite_model = GraniteTimeSeriesClassifier(
            num_horizons=len(FORECAST_HORIZONS),
            device=self.device,
            lora_config=lora_config
        )
        
        model = EnhancedHybridModel(
            granite_model=granite_model,
            stat_feature_dim=len(self.feature_cols),
            hidden_dim=128,
            dropout=0.3
        ).to(self.device)
        
        # è¨“ç·´å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"  Total parameters: {total_params:,}")
        print(f"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
        
        # Focal Loss
        pos_weight = (len(train_dataset) - train_dataset.labels.sum()) / train_dataset.labels.sum()
        alpha = 1.0 / (1.0 + pos_weight)
        criterion = FocalLoss(alpha=alpha, gamma=3.0)
        print(f"  Focal Loss: alpha={alpha:.3f}, gamma=3.0")
        
        # Optimizer & Scheduler
        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01, eps=1e-8)
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr/10)
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ç”¨ã®æœ€å¤§ãƒãƒ«ãƒ 
        max_grad_norm = 1.0
        
        # è¨“ç·´ãƒ«ãƒ¼ãƒ—
        print(f"\nğŸš€ Training for {epochs} epochs...")
        best_f1 = 0.0
        history = {'train_loss': [], 'test_loss': [], 'test_f1': []}
        
        for epoch in range(epochs):
            # Train
            model.train()
            train_loss = 0.0
            
            for batch in train_loader:
                sequences = batch['sequence'].to(self.device)
                features = batch['features'].to(self.device)
                labels = batch['label'].squeeze().to(self.device)
                
                optimizer.zero_grad()
                outputs = model(sequences, features)
                
                # NaN/Infãƒã‚§ãƒƒã‚¯ï¼ˆå‡ºåŠ›ï¼‰
                if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                    print(f"  Warning: outputs contain NaN/Inf at batch, skipping...")
                    continue
                
                loss = criterion(outputs, labels)
                
                # NaN/Infãƒã‚§ãƒƒã‚¯ï¼ˆæå¤±ï¼‰
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"  Warning: loss is NaN/Inf at batch, skipping...")
                    continue
                
                loss.backward()
                
                # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                
                optimizer.step()
                
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # Validation
            model.eval()
            test_loss = 0.0
            all_preds = []
            all_labels = []
            
            with torch.no_grad():
                for batch in test_loader:
                    sequences = batch['sequence'].to(self.device)
                    features = batch['features'].to(self.device)
                    labels = batch['label'].squeeze().to(self.device)
                    
                    outputs = model(sequences, features)
                    loss = criterion(outputs, labels)
                    
                    test_loss += loss.item()
                    all_preds.extend(outputs.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
            
            test_loss /= len(test_loader)
            
            # Metrics
            all_preds = np.array(all_preds)
            all_labels = np.array(all_labels)
            
            precision, recall, thresholds = precision_recall_curve(all_labels, all_preds)
            f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1] + 1e-10)
            best_idx = np.argmax(f1_scores)
            best_threshold = thresholds[best_idx]
            best_f1_score = f1_scores[best_idx]
            
            pred_binary = (all_preds > best_threshold).astype(int)
            accuracy = accuracy_score(all_labels, pred_binary)
            
            history['train_loss'].append(train_loss)
            history['test_loss'].append(test_loss)
            history['test_f1'].append(best_f1_score)
            
            print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | "
                  f"Test Loss: {test_loss:.4f} | F1: {best_f1_score:.4f} | "
                  f"Acc: {accuracy:.4f} | Threshold: {best_threshold:.3f}")
            
            # Best modelä¿å­˜
            if best_f1_score > best_f1:
                best_f1 = best_f1_score
                best_model_state = model.state_dict()
                best_threshold_value = best_threshold
            
            scheduler.step()
        
        # Best modelã‚’ãƒ­ãƒ¼ãƒ‰
        model.load_state_dict(best_model_state)
        
        # æœ€çµ‚è©•ä¾¡
        print(f"\nğŸ“Š Final Evaluation (best F1={best_f1:.4f})...")
        model.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in test_loader:
                sequences = batch['sequence'].to(self.device)
                features = batch['features'].to(self.device)
                labels = batch['label'].squeeze()
                
                outputs = model(sequences, features)
                all_preds.extend(outputs.cpu().numpy())
                all_labels.extend(labels.numpy())
        
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        pred_binary = (all_preds > best_threshold_value).astype(int)
        
        metrics = {
            'horizon': horizon,
            'threshold': best_threshold_value,
            'accuracy': accuracy_score(all_labels, pred_binary),
            'precision': precision_score(all_labels, pred_binary, zero_division=0),
            'recall': recall_score(all_labels, pred_binary, zero_division=0),
            'f1': f1_score(all_labels, pred_binary, zero_division=0),
            'roc_auc': roc_auc_score(all_labels, all_preds),
            'pr_auc': average_precision_score(all_labels, all_preds)
        }
        
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall: {metrics['recall']:.4f}")
        print(f"  F1-Score: {metrics['f1']:.4f}")
        print(f"  ROC-AUC: {metrics['roc_auc']:.4f}")
        print(f"  PR-AUC: {metrics['pr_auc']:.4f}")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_dir = MODEL_ROOT / "hybrid_model_v2.1"
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = model_dir / f"pytorch_model_{horizon}d.pt"
        torch.save({
            'model_state_dict': model.state_dict(),
            'threshold': best_threshold_value,
            'metrics': metrics,
            'history': history
        }, model_path)
        print(f"âœ“ Model saved: {model_path}")
        
        self.models[horizon] = model
        self.results[horizon] = {
            'metrics': metrics,
            'predictions': all_preds,
            'labels': all_labels,
            'history': history
        }
        
        return model, metrics
    
    def plot_training_history(self):
        """è¨“ç·´å±¥æ­´ã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        print(f"\nğŸ“ˆ Plotting training history...")
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        for idx, horizon in enumerate(FORECAST_HORIZONS):
            if horizon not in self.results:
                continue
            
            history = self.results[horizon]['history']
            epochs = range(1, len(history['train_loss']) + 1)
            
            ax = axes[idx]
            ax2 = ax.twinx()
            
            # Loss
            ax.plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)
            ax.plot(epochs, history['test_loss'], 'r-', label='Test Loss', linewidth=2)
            ax.set_xlabel('Epoch', fontsize=12)
            ax.set_ylabel('Loss', fontsize=12, color='b')
            ax.tick_params(axis='y', labelcolor='b')
            ax.legend(loc='upper left')
            ax.grid(True, alpha=0.3)
            
            # F1 Score
            ax2.plot(epochs, history['test_f1'], 'g-', label='Test F1', linewidth=2)
            ax2.set_ylabel('F1 Score', fontsize=12, color='g')
            ax2.tick_params(axis='y', labelcolor='g')
            ax2.legend(loc='upper right')
            
            ax.set_title(f'{horizon}d Horizon Training', fontsize=14, fontweight='bold')
        
        plt.suptitle('Enhanced Hybrid Model v2.1 Training History', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
        save_path = RESULTS_ROOT / f'training_history_v2.1_{timestamp}.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"  âœ“ Saved: {save_path}")
        plt.show()
    
    def save_summary(self):
        """çµæœã‚µãƒãƒªãƒ¼ã®ä¿å­˜"""
        print(f"\nğŸ’¾ Saving results summary...")
        
        summary = []
        for horizon in FORECAST_HORIZONS:
            if horizon in self.results:
                metrics = self.results[horizon]['metrics']
                summary.append(metrics)
        
        summary_df = pd.DataFrame(summary)
        
        model_dir = MODEL_ROOT / "hybrid_model_v2.1"
        summary_path = model_dir / "metrics_summary_v2.1.csv"
        summary_df.to_csv(summary_path, index=False)
        
        print(f"âœ“ Summary saved: {summary_path}")
        print(f"\n{summary_df}")
    
    def run(self, epochs: int = 10, batch_size: int = 128, lr: float = 1e-4):
        """å®Œå…¨å®Ÿè¡Œ"""
        print("\n" + "="*70)
        print("Enhanced Hybrid Model v2.1 Training")
        print("="*70)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        self.load_data()
        
        # å„ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã§è¨“ç·´
        for horizon in FORECAST_HORIZONS:
            self.train_horizon(horizon, epochs=epochs, batch_size=batch_size, lr=lr)
        
        # å¯è¦–åŒ–
        self.plot_training_history()
        
        # ã‚µãƒãƒªãƒ¼ä¿å­˜
        self.save_summary()
        
        print("\nâœ… Training completed!")
        print(f"ğŸ“ Models saved to: {MODEL_ROOT / 'hybrid_model_v2.1'}")


def main():
    trainer = HybridTrainerV2_1()
    # Encoderã¯å›ºå®šãªã®ã§ã€ã‚ˆã‚Šé«˜é€Ÿã«è¨“ç·´å¯èƒ½
    trainer.run(epochs=20, batch_size=128, lr=5e-4)


if __name__ == "__main__":
    main()
